{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "555700a6",
      "metadata": {
        "id": "555700a6"
      },
      "source": [
        "\n",
        "Ans1:\n",
        "\n",
        "Detectron2 is Facebook AI Research's (FAIR) second-generation object detection and segmentation library. It is implemented in PyTorch and provides a flexible, modular codebase for training, evaluating, and deploying state-of-the-art computer vision models such as Faster R-CNN etc.\n",
        " It is Different from others in terms of\n",
        "\n",
        " 1. Language and backend\n",
        " 2. Modularity\n",
        " 3. Performance and Analysis\n",
        " 4. Flexibilty\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b4e0d65b",
      "metadata": {
        "id": "b4e0d65b"
      },
      "source": [
        "Ans2:\n",
        "\n",
        "Data annotation is one of the most critical steps when training any supervised computer vision model, including object detection and segmentation models in Detectron2. The quality, consistency, and completeness of the annotations directly influence the accuracy, robustness, and generalization ability of the resulting model.\n",
        "\n",
        "It is Important because\n",
        "1. Provides Supervised Training Signal\n",
        "2. Determines Model Accuracy and Generalization\n",
        "3. Helps Model Understand Edge Cases\n",
        "4. Enables Fine-Grained Tasks\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "757947a3",
      "metadata": {
        "id": "757947a3"
      },
      "source": [
        "Ans3:\n",
        "\n",
        "1. Data collection & annotation\n",
        "   - Capture images representing the deployment domain (angles, lighting, occlusion).\n",
        "   - Annotate with bounding boxes/masks/keypoints as required and export to COCO or convert to COCO.\n",
        "\n",
        "2. Prepare dataset for Detectron2\n",
        "   - Register datasets with `DatasetCatalog` and `MetadataCatalog`.\n",
        "   - Example helper function returns a list of dicts (`image`, `annotations`) matching Detectron2 format.\n",
        "\n",
        "3. Select a base config & model\n",
        "   - Choose from Detectron2 model zoo (e.g., `COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml`, `COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml`).\n",
        "   - Use `model_zoo.get_config_file` and `get_checkpoint_url` to initialize weights.\n",
        "\n",
        "4. Modify configuration\n",
        "   - Set `NUM_CLASSES` to the number of object categories.\n",
        "   - Update dataset names: `DATASETS.TRAIN` and `DATASETS.TEST`.\n",
        "   - Configure `SOLVER` parameters: `BASE_LR`, `STEPS`, `MAX_ITER`, `IMS_PER_BATCH`.\n",
        "   - Set `OUTPUT_DIR` and `MODEL.WEIGHTS` (pretrained checkpoint or from last checkpoint).\n",
        "   - Optionally enable mixed precision (`AMP`) and distributed training settings.\n",
        "\n",
        "5. Create a Trainer and start training\n",
        "   - Use `DefaultTrainer` for most cases. For custom losses or hooks, create a subclass of `DefaultTrainer`.\n",
        "   - Example: `trainer = DefaultTrainer(cfg); trainer.resume_or_load(resume=False); trainer.train()`\n",
        "\n",
        "6. Monitoring and debugging\n",
        "   - Use the logger output and TensorBoard to inspect losses, LR schedule, and time per iteration.\n",
        "   - Visualize model predictions on a validation subset periodically to spot systematic errors.\n",
        "\n",
        "7. Evaluation\n",
        "   - Use Detectron2's `COCOEvaluator` or `DatasetEvaluator` to compute metrics like mAP, AR.\n",
        "   - Save best checkpoints (by validation mAP) and analyze precision-recall curves.\n",
        "\n",
        "8. Fine-tuning & hyperparameter search\n",
        "   - Adjust learning rate, batch size, augmentation, or use a larger backbone if underfitting.\n",
        "   - If overfitting, add augmentation (random cropping, color jitter), regularize, or get more data.\n",
        "\n",
        "9. Exporting / Deployment\n",
        "   - Export to TorchScript or ONNX for production deployment, or keep PyTorch weights for server inference.\n",
        "   - Optionally prune or quantize the model for edge devices.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "699f8134",
      "metadata": {
        "id": "699f8134"
      },
      "source": [
        "Ans4:\n",
        "\n",
        "\n",
        "1. Intersection over Union (IoU) : it measures how well the predicted box/mask overlaps with the ground-truth. An IoU of 1.0 is perfect overlap; 0.0 is no overlap.\n",
        "\n",
        "2. Precision and Recall\n",
        "   - Precision: proportion of predicted positives that are correct: TP / (TP + FP).\n",
        "   - Recall: proportion of ground-truth positives that are detected: TP / (TP + FN)\n",
        "\n",
        "3. Precision-Recall (PR) Curve\n",
        "   - A PR curve plots precision vs recall as the confidence threshold varies.\n",
        "   - The area under the PR curve (Average Precision, AP) summarizes the curve into a single number.\n",
        "\n",
        "4. Average Precision (AP) and mean Average Precision (mAP)\n",
        "   - AP: computed per class (or per IoU threshold) as the area under the PR curve. Various interpolation methods exist; COCO uses averaging over multiple recall points.\n",
        "   - mAP: mean of AP values across classes (and in COCO, across multiple IoU thresholds). It gives a single-score summary.\n",
        "\n",
        "5. COCO evaluation specifics\n",
        "   - COCO uses mAP averaged over IoU thresholds from 0.50 to 0.95 in steps of 0.05 — reported as `AP@[.5:.95]` (commonly written simply as `AP` in COCO reports).\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c60b7941",
      "metadata": {
        "id": "c60b7941"
      },
      "source": [
        "Ans5:\n",
        "\n",
        "1. Detectron2\n",
        "\n",
        "-Developed by Facebook AI Research (FAIR).\n",
        "\n",
        "-Built on PyTorch.\n",
        "\n",
        "-Designed for both research and production use, with a modular, flexible architecture and support for many advanced computer vision tasks (object detection, instance segmentation, panoptic segmentation, keypoint detection, etc.).\n",
        "\n",
        "2. TensorFlow Object Detection API (TFOD2)\n",
        "\n",
        "-Developed by Google / TensorFlow community.\n",
        "\n",
        "-Built on TensorFlow 2.x.\n",
        "\n",
        "-Provides many out-of-the-box model architectures and pipelines; often used for production, quick experimentation, and easier deployment to TensorFlow-serving, mobile (TFLite), and edge."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "61d62549",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "61d62549",
        "outputId": "86d4bf2c-be8d-4e24-9cbd-84808e4a1baa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.9.0+cu126)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch) (3.6)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.5.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement detectron2 (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for detectron2\u001b[0m\u001b[31m\n",
            "\u001b[0mCollecting git+https://github.com/facebookresearch/detectron2.git\n",
            "  Cloning https://github.com/facebookresearch/detectron2.git to /tmp/pip-req-build-kkgt3a4w\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/facebookresearch/detectron2.git /tmp/pip-req-build-kkgt3a4w\n",
            "  Resolved https://github.com/facebookresearch/detectron2.git to commit fd27788985af0f4ca800bca563acdb700bb890e2\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: Pillow>=7.1 in /usr/local/lib/python3.12/dist-packages (from detectron2==0.6) (11.3.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from detectron2==0.6) (3.10.0)\n",
            "Requirement already satisfied: pycocotools>=2.0.2 in /usr/local/lib/python3.12/dist-packages (from detectron2==0.6) (2.0.10)\n",
            "Requirement already satisfied: termcolor>=1.1 in /usr/local/lib/python3.12/dist-packages (from detectron2==0.6) (3.2.0)\n",
            "Collecting yacs>=0.1.8 (from detectron2==0.6)\n",
            "  Downloading yacs-0.1.8-py3-none-any.whl.metadata (639 bytes)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.12/dist-packages (from detectron2==0.6) (0.9.0)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.12/dist-packages (from detectron2==0.6) (3.1.2)\n",
            "Requirement already satisfied: tqdm>4.29.0 in /usr/local/lib/python3.12/dist-packages (from detectron2==0.6) (4.67.1)\n",
            "Requirement already satisfied: tensorboard in /usr/local/lib/python3.12/dist-packages (from detectron2==0.6) (2.19.0)\n",
            "Collecting fvcore<0.1.6,>=0.1.5 (from detectron2==0.6)\n",
            "  Downloading fvcore-0.1.5.post20221221.tar.gz (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.2/50.2 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting iopath<0.1.10,>=0.1.7 (from detectron2==0.6)\n",
            "  Downloading iopath-0.1.9-py3-none-any.whl.metadata (370 bytes)\n",
            "Requirement already satisfied: omegaconf<2.4,>=2.1 in /usr/local/lib/python3.12/dist-packages (from detectron2==0.6) (2.3.0)\n",
            "Collecting hydra-core>=1.1 (from detectron2==0.6)\n",
            "  Downloading hydra_core-1.3.2-py3-none-any.whl.metadata (5.5 kB)\n",
            "Collecting black (from detectron2==0.6)\n",
            "  Downloading black-25.11.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (85 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.2/85.2 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from detectron2==0.6) (25.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from fvcore<0.1.6,>=0.1.5->detectron2==0.6) (2.0.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from fvcore<0.1.6,>=0.1.5->detectron2==0.6) (6.0.3)\n",
            "Requirement already satisfied: antlr4-python3-runtime==4.9.* in /usr/local/lib/python3.12/dist-packages (from hydra-core>=1.1->detectron2==0.6) (4.9.3)\n",
            "Collecting portalocker (from iopath<0.1.10,>=0.1.7->detectron2==0.6)\n",
            "  Downloading portalocker-3.2.0-py3-none-any.whl.metadata (8.7 kB)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from black->detectron2==0.6) (8.3.1)\n",
            "Collecting mypy-extensions>=0.4.3 (from black->detectron2==0.6)\n",
            "  Downloading mypy_extensions-1.1.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Collecting pathspec>=0.9.0 (from black->detectron2==0.6)\n",
            "  Downloading pathspec-0.12.1-py3-none-any.whl.metadata (21 kB)\n",
            "Requirement already satisfied: platformdirs>=2 in /usr/local/lib/python3.12/dist-packages (from black->detectron2==0.6) (4.5.0)\n",
            "Collecting pytokens>=0.3.0 (from black->detectron2==0.6)\n",
            "  Downloading pytokens-0.3.0-py3-none-any.whl.metadata (2.0 kB)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->detectron2==0.6) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->detectron2==0.6) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->detectron2==0.6) (4.60.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->detectron2==0.6) (1.4.9)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->detectron2==0.6) (3.2.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib->detectron2==0.6) (2.9.0.post0)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.12/dist-packages (from tensorboard->detectron2==0.6) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.12/dist-packages (from tensorboard->detectron2==0.6) (1.76.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.12/dist-packages (from tensorboard->detectron2==0.6) (3.10)\n",
            "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /usr/local/lib/python3.12/dist-packages (from tensorboard->detectron2==0.6) (5.29.5)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorboard->detectron2==0.6) (75.2.0)\n",
            "Requirement already satisfied: six>1.9 in /usr/local/lib/python3.12/dist-packages (from tensorboard->detectron2==0.6) (1.17.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from tensorboard->detectron2==0.6) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from tensorboard->detectron2==0.6) (3.1.3)\n",
            "Requirement already satisfied: typing-extensions~=4.12 in /usr/local/lib/python3.12/dist-packages (from grpcio>=1.48.2->tensorboard->detectron2==0.6) (4.15.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.12/dist-packages (from werkzeug>=1.0.1->tensorboard->detectron2==0.6) (3.0.3)\n",
            "Downloading hydra_core-1.3.2-py3-none-any.whl (154 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.5/154.5 kB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading iopath-0.1.9-py3-none-any.whl (27 kB)\n",
            "Downloading yacs-0.1.8-py3-none-any.whl (14 kB)\n",
            "Downloading black-25.11.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (1.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m63.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mypy_extensions-1.1.0-py3-none-any.whl (5.0 kB)\n",
            "Downloading pathspec-0.12.1-py3-none-any.whl (31 kB)\n",
            "Downloading pytokens-0.3.0-py3-none-any.whl (12 kB)\n",
            "Downloading portalocker-3.2.0-py3-none-any.whl (22 kB)\n",
            "Building wheels for collected packages: detectron2, fvcore\n",
            "  Building wheel for detectron2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for detectron2: filename=detectron2-0.6-cp312-cp312-linux_x86_64.whl size=7085041 sha256=87d677b2ef222d328a1464d0bef14352ea4d7df013c22bba7941fdfb8aee3b98\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-n8jb24st/wheels/d3/6e/bd/1969578f1456a6be2d6f083da65c669f450b23b8f3d1ac14c1\n",
            "  Building wheel for fvcore (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fvcore: filename=fvcore-0.1.5.post20221221-py3-none-any.whl size=61397 sha256=f88ceb843a2dd5edcb14224a003a67d5183f3c9c4c971e44be6714dd7a371d3d\n",
            "  Stored in directory: /root/.cache/pip/wheels/ed/9f/a5/e4f5b27454ccd4596bd8b62432c7d6b1ca9fa22aef9d70a16a\n",
            "Successfully built detectron2 fvcore\n",
            "Installing collected packages: yacs, pytokens, portalocker, pathspec, mypy-extensions, iopath, hydra-core, black, fvcore, detectron2\n",
            "Successfully installed black-25.11.0 detectron2-0.6 fvcore-0.1.5.post20221221 hydra-core-1.3.2 iopath-0.1.9 mypy-extensions-1.1.0 pathspec-0.12.1 portalocker-3.2.0 pytokens-0.3.0 yacs-0.1.8\n",
            "PyTorch version: 2.9.0+cu126\n",
            "CUDA available: True\n",
            "Detectron2 version: 0.6\n",
            "Loaded config for: GeneralizedRCNN\n"
          ]
        }
      ],
      "source": [
        " #Ans6:\n",
        "\n",
        "\n",
        "! pip install torch\n",
        "! pip install 'git+https://github.com/facebookresearch/detectron2.git'\n",
        "\n",
        "import torch\n",
        "import detectron2\n",
        "from detectron2.utils.logger import setup_logger\n",
        "setup_logger()\n",
        "print('PyTorch version:', torch.__version__)\n",
        "print('CUDA available:', torch.cuda.is_available())\n",
        "print('Detectron2 version:', detectron2.__version__)\n",
        "\n",
        "\n",
        "from detectron2 import model_zoo\n",
        "from detectron2.config import get_cfg\n",
        "cfg = get_cfg()\n",
        "cfg.merge_from_file(model_zoo.get_config_file(\"COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml\"))\n",
        "print('Loaded config for:', cfg.MODEL.META_ARCHITECTURE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "03848e16",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "03848e16",
        "outputId": "20025359-fe0d-4372-b770-8aaf0ef62833"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wrote COCO file with 0 images and 0 annotations to annotations_coco.json\n"
          ]
        }
      ],
      "source": [
        "#Ans7:\n",
        "\n",
        "import os, json, glob\n",
        "from PIL import Image\n",
        "\n",
        "labelme_dir = 'labelme_jsons'\n",
        "output_coco = 'annotations_coco.json'\n",
        "\n",
        "categories = [\n",
        "    {'id': 1, 'name': 'animal', 'supercategory': 'animal'}\n",
        "]\n",
        "\n",
        "images = []\n",
        "annotations = []\n",
        "ann_id = 1\n",
        "for idx, json_file in enumerate(glob.glob(os.path.join(labelme_dir, '*.json'))):\n",
        "    with open(json_file, 'r') as f:\n",
        "        data = json.load(f)\n",
        "    img_path = data.get('imagePath') or data.get('imagePath', os.path.splitext(json_file)[0]+'.jpg')\n",
        "\n",
        "    try:\n",
        "        im = Image.open(os.path.join(os.path.dirname(json_file), img_path))\n",
        "        width, height = im.size\n",
        "    except Exception:\n",
        "\n",
        "        width, height = data.get('imageWidth'), data.get('imageHeight')\n",
        "    image_id = idx + 1\n",
        "    images.append({'file_name': img_path, 'height': height, 'width': width, 'id': image_id})\n",
        "\n",
        "    for shape in data.get('shapes', []):\n",
        "        label = shape.get('label', 'animal')\n",
        "        points = shape.get('points', [])\n",
        "\n",
        "        seg = [p for point in points for p in point]\n",
        "\n",
        "        xs = [p[0] for p in points]\n",
        "        ys = [p[1] for p in points]\n",
        "        x_min, y_min = min(xs), min(ys)\n",
        "        w, h = max(xs)-x_min, max(ys)-y_min\n",
        "        annotation = {\n",
        "            'id': ann_id,\n",
        "            'image_id': image_id,\n",
        "            'category_id': 1,\n",
        "            'segmentation': [seg],\n",
        "            'bbox': [x_min, y_min, w, h],\n",
        "            'iscrowd': 0,\n",
        "            'area': w*h\n",
        "        }\n",
        "        annotations.append(annotation)\n",
        "        ann_id += 1\n",
        "\n",
        "coco_dict = {\n",
        "    'images': images,\n",
        "    'annotations': annotations,\n",
        "    'categories': categories\n",
        "}\n",
        "\n",
        "with open(output_coco, 'w') as f:\n",
        "    json.dump(coco_dict, f)\n",
        "print('Wrote COCO file with', len(images), 'images and', len(annotations), 'annotations to', output_coco)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "5359d1dd",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5359d1dd",
        "outputId": "b8a7a35d-ff28-4337-8601-5b6ce4c5cf0d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Configured model weights: https://dl.fbaipublicfiles.com/detectron2/COCO-Detection/faster_rcnn_R_50_FPN_3x/137849458/model_final_280758.pkl\n",
            "Output dir: ./output/\n"
          ]
        }
      ],
      "source": [
        "#Ans8:\n",
        "\n",
        "from detectron2.config import get_cfg\n",
        "from detectron2 import model_zoo\n",
        "import os\n",
        "\n",
        "cfg = get_cfg()\n",
        "\n",
        "config_file = \"COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml\"\n",
        "cfg.merge_from_file(model_zoo.get_config_file(config_file))\n",
        "\n",
        "cfg.DATASETS.TRAIN = (\"my_coco_train\",)\n",
        "cfg.DATASETS.TEST = (\"my_coco_val\",)\n",
        "\n",
        "cfg.OUTPUT_DIR = \"./output/\"\n",
        "\n",
        "cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(config_file)\n",
        "\n",
        "\n",
        "cfg.SOLVER.IMS_PER_BATCH = 4\n",
        "cfg.SOLVER.BASE_LR = 0.00025\n",
        "cfg.SOLVER.MAX_ITER = 10000\n",
        "cfg.SOLVER.STEPS = (7000, 9000)\n",
        "cfg.SOLVER.WARMUP_ITERS = 1000\n",
        "\n",
        "\n",
        "cfg.MODEL.ROI_HEADS.NUM_CLASSES = 3\n",
        "\n",
        "\n",
        "os.makedirs(cfg.OUTPUT_DIR, exist_ok=True)\n",
        "print('Configured model weights:', cfg.MODEL.WEIGHTS)\n",
        "print('Output dir:', cfg.OUTPUT_DIR)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "ae6422db",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 411
        },
        "id": "ae6422db",
        "outputId": "1f95fd29-1592-4d28-e69f-f826a525c7fe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[12/07 11:21:20 d2.checkpoint.detection_checkpoint]: [DetectionCheckpointer] Loading from ./output/model_final_280758.pkl ...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "UnpicklingError",
          "evalue": "could not find MARK",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mUnpicklingError\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-832274631.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mcfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMODEL\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDEVICE\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"cuda\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mpredictor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDefaultPredictor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/detectron2/engine/defaults.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, cfg)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    319\u001b[0m         \u001b[0mcheckpointer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDetectionCheckpointer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 320\u001b[0;31m         \u001b[0mcheckpointer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMODEL\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mWEIGHTS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    321\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m         self.aug = T.ResizeShortestEdge(\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/detectron2/checkpoint/detection_checkpoint.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(self, path, *args, **kwargs)\u001b[0m\n\u001b[1;32m     60\u001b[0m             \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsed_url\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_replace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgeturl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# remove query from filename\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_local_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mneed_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/fvcore/common/checkpoint.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(self, path, checkpointables)\u001b[0m\n\u001b[1;32m    153\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Checkpoint {} not found!\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 155\u001b[0;31m         \u001b[0mcheckpoint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_load_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    156\u001b[0m         \u001b[0mincompatible\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_load_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m         if (\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/detectron2/checkpoint/detection_checkpoint.py\u001b[0m in \u001b[0;36m_load_file\u001b[0;34m(self, filename)\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".pkl\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mPathManager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"latin1\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;34m\"model\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"__author__\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m                 \u001b[0;31m# file is in Detectron2 model zoo format\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mUnpicklingError\u001b[0m: could not find MARK"
          ]
        }
      ],
      "source": [
        "# Ans9:\n",
        "\n",
        "import cv2\n",
        "from detectron2.engine import DefaultPredictor\n",
        "from detectron2.config import get_cfg\n",
        "from detectron2.utils.visualizer import Visualizer, ColorMode\n",
        "from detectron2 import model_zoo\n",
        "\n",
        "\n",
        "cfg = get_cfg()\n",
        "cfg.merge_from_file(model_zoo.get_config_file(\"COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml\"))\n",
        "cfg.MODEL.WEIGHTS = \"./output/model_final_280758.pkl\"\n",
        "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5\n",
        "cfg.MODEL.ROI_HEADS.NUM_CLASSES = 3\n",
        "cfg.MODEL.DEVICE = \"cuda\"\n",
        "\n",
        "predictor = DefaultPredictor(cfg)\n",
        "\n",
        "\n",
        "image_path = 'test_image.jpg'\n",
        "im = cv2.imread(image_path)\n",
        "outputs = predictor(im)\n",
        "\n",
        "\n",
        "instances = outputs['instances'].to('cpu')\n",
        "print('Detected', len(instances), 'instances')\n",
        "\n",
        "\n",
        "v = Visualizer(im[:, :, ::-1], metadata=None, scale=1.0, instance_mode=ColorMode.IMAGE)\n",
        "out = v.draw_instance_predictions(instances)\n",
        "result_bgr = out.get_image()[:, :, ::-1]\n",
        "cv2.imwrite('inference_result.jpg', result_bgr)\n",
        "print('Saved inference visualization to inference_result.jpg')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a601fa89",
      "metadata": {
        "id": "a601fa89"
      },
      "source": [
        " Ans10:\n",
        "\n",
        "\n",
        "**End-to-end pipeline for wildlife monitoring**\n",
        "\n",
        "1. **Problem scoping**\n",
        "   - Define target species, expected tasks (detection, instance segmentation, tracking), and performance metrics (precision/recall, latency).\n",
        "   - Decide deployment: edge (camera with GPU/accelerator) vs central server.\n",
        "\n",
        "2. **Data collection**\n",
        "   - Collect images and video from camera traps, drones, or field cameras. Gather diverse conditions: daytime, nighttime, different seasons.\n",
        "   - Capture multiple viewpoints, distances, and occlusion scenarios.\n",
        "   - If possible, collect thermal or infrared imagery for night detection.\n",
        "\n",
        "3. **Annotation**\n",
        "   - Use CVAT/LabelMe/LabelImg for bounding boxes and segmentation masks.\n",
        "   - Create a labeling guideline covering partial occlusions, juveniles vs adults, and bounding rules.\n",
        "   - Consider hierarchical labels if you need coarse/fine taxonomy.\n",
        "\n",
        "4. **Data preprocessing & augmentation**\n",
        "   - Resize and normalize images consistent with the model.\n",
        "   - Augmentations: random crops, flips, color jitter, blur, contrast adjustments, cutout, synthetic low-light augmentations for nighttime.\n",
        "   - For small-object-heavy datasets, ensure augmentations preserve small object visibility.\n",
        "\n",
        "5. **Model selection**\n",
        "   - For detection+segmentation: Mask R-CNN or a hybrid model in Detectron2.\n",
        "   - For speed-constrained deployments: RetinaNet, EfficientDet, or YOLO-family models (may require other repos or conversion to Detectron2-compatible format).\n",
        "\n",
        "6. **Training strategy**\n",
        "   - Pretrain on COCO or related datasets, then fine-tune on wildlife dataset.\n",
        "   - Use class-balanced sampling, focal loss (for class imbalance), and multi-scale training.\n",
        "   - Use validation splits that reflect deployment distribution (e.g., separate by camera location).\n",
        "\n",
        "7. **Handling occlusion**\n",
        "   - **Data level:** include many occlusion examples, annotate partially visible animals, and use instance masks if possible.\n",
        "   - **Model & temporal cues:** use video frames and temporal smoothing or tubelets; incorporate tracking that links partial detections across frames.\n",
        "   - **Multi-view fusion:** if you have multiple cameras, fuse detections spatially when possible.\n",
        "   - **Advanced models:** consider architectures or modules that reason about context (relation networks) or use part-based detection.\n",
        "\n",
        "8. **Handling nighttime detection**\n",
        "   - **Sensor-level:** use IR/thermal cameras to capture heat signatures.\n",
        "   - **Model-level:** include IR/thermal images in the training set; perform domain adaptation or train separate specialized models for nighttime.\n",
        "   - **Preprocessing:** apply low-light enhancement or denoising as a preprocessing step; use histogram equalization.\n",
        "\n",
        "9. **Tracking**\n",
        "   - Combine detection outputs with a tracker (DeepSORT, ByteTrack, or SORT) to maintain identities across frames.\n",
        "   - Tracking helps with counting, behavior analysis, and recovering missed detections (occlusion recovery via temporal continuity).\n",
        "\n",
        "10. **Evaluation**\n",
        "   - Evaluate detection per-species using mAP and per-size metrics.\n",
        "   - Evaluate tracking using MOT metrics (MOTA, IDF1, ID switches).\n",
        "\n",
        "11. **Deployment**\n",
        "   - **Edge deployment:** convert model to TorchScript or ONNX, optimize with TensorRT or ONNX Runtime, and deploy on NVIDIA Jetson or AWS Panorama.\n",
        "   - **Server deployment:** host as a REST/gRPC service using TorchServe or NVIDIA Triton for batch/streaming inference.\n",
        "   - Implement batching, rate-limiting, and asynchronous pipelines for video processing.\n",
        "\n",
        "12. **Monitoring & continuous improvement**\n",
        "   - Log model predictions and uncertain cases; periodically retrain with new labeled data (active learning loop).\n",
        "   - Monitor drift in environment (seasonal changes) and refresh datasets.\n",
        "\n",
        "13. **Ethics & privacy**\n",
        "   - Ensure camera placements comply with local laws and avoid capturing people where privacy concerns arise.\n",
        "   - Secure collected data and annotate/deidentify human occurrences if they are present.\n",
        "\n",
        "**Concrete techniques for robustness**\n",
        "- **Ensemble models:** combine detectors with different strengths (e.g., one tuned for small objects, one for occlusion resilience).\n",
        "- **Temporal aggregation:** use temporal smoothing on detections (Kalman filters or tracking-based aggregation).\n",
        "- **Domain adaptation:** if switching between camera types or sensors, use unsupervised domain adaptation or fine-tuning on a small labeled set.\n",
        "\n",
        "**Summary:** Building a wildlife monitoring system requires careful dataset construction, sensor choice (visible vs thermal), model selection and fine-tuning, and incorporation of temporal tracking. Address occlusion with data and tracking; handle nighttime by using thermal/IR sensors and augmentation. Finally, plan for deployment constraints (latency, compute) and an active learning loop for continual improvements.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}